\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Example of structured dataset $\mathcal {D} \in \mathbb {R}^2$. \textbf {Normal} points are shown in \textbf {orange}, while \textbf {anomalies} are shown in \textbf {blue}. The pattern in the dataset is a sinusoidal function $f(x) = y = sin(x)cos(x)$. Anomalies are random points that do not follow any pattern, laying in the same space of the normal points.\relax }}{2}{figure.caption.24}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of a 2D line.\relax }}{5}{figure.caption.32}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a 3D plane.\relax }}{7}{figure.caption.37}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a neural network. The various $a_{ij}$ represent the \textit {j-th} neuron of \textit {i-th} layer. Weights $w_{kj}$ represent the weight of the connection from neuron \textit {k} of the preceding layer to neuron \textit {j} of the current layer.\relax }}{8}{figure.caption.39}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of an architecture for an auto-encoder. The input (and output) is \textit {2-dimensional}, thus the latent space must be \textit {1-dimensional}.\relax }}{10}{figure.caption.53}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Ideal structure of SOM. Each element of the input vector, $x_i$, is connected to each neuron of the output layer, displaced as a grid.\relax }}{12}{figure.caption.59}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Image that shows the effect of the neighboring function. At time 0 several neighbors are considered, while as the time passes ever less neighbors are updated. The color represents the update's intensity; the higher the intensity, the closer the weight is to the BMU.\relax }}{13}{figure.caption.65}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces An example of a Receiver Operating Characteristic curve, showing both the ideal curve and the curve from a generic model, with good results.\relax }}{17}{figure.caption.74}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textit {reach-dist}($\textbf {p}_1$, $\textbf {o}$) and \textit {reach-dist}($\textbf {p}_2$, $\textbf {o}$) for $k=4$\relax }}{21}{figure.caption.83}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Two examples of how a point can be isolated. If it is a normal point, figure \ref {subfig:ifor-normal}, it is harder to isolate and more splits are required. If instead it is an anomaly, figure \ref {subfig:ifor-anom}, it is easier to isolate and less splits are required.\relax }}{24}{figure.caption.93}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Example of a Pi-Tree with branching factor $b = 3$ and height limit $l = 3$, built from a set of points in $\mathbb {R}^2$. Every region is recursively split in $b$ sub-regions and can be noted that the most isolated samples fall in leaves at lowest heights, such as \textit {a} and \textit {d} cells.\relax }}{31}{figure.caption.109}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of a linear approximation on a generic function $y = f(x)$. In blue the true function, in green the tangent at the point $(p, f(p))$ that is marked in red.\relax }}{40}{figure.caption.128}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Example of a plane, $\alpha $, the is tangent to the surface $f(x, y, z)$ at the point $\textbf {p}$.\relax }}{42}{figure.caption.135}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces This figure shows the utility of the constraint on the latent representation's number of dimensions. In image \ref {subfig:mlp}, a Multi Layer Perceptron (MLP) network has learned to reconstruct the input vector generating a new vector in the same space; the problem is that this network didn't generate any latent representation with fewer dimension, thus it has learnt the identity matrix. On the contrary, in image \ref {subfig:ae1}, an Auto-Encoder with a latent representation with less dimensions than the input space has learned a new representation of the input dataset, not the identity matrix.\relax }}{43}{figure.caption.136}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Structure of the network used in this section. The pedices of the weights are the starting neuron's index and the connected neuron's index. So, for example, weight $w_{02}$ goes from neuron $n_0$ to neuron $n_2$, or weight $w_{23}$ goes from neuron $n_2$ to neuron $n_3$. The biases have the same index to which they belong; for example, bias $b_4$ is the bias of neuron $n_4$.\relax }}{45}{figure.caption.142}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The structure of the network used in this section. The convention for naming the weights and biases is the same used in figure \ref {fig:ae_212}.\relax }}{48}{figure.caption.160}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Two examples of how SOM weights can be arranged. Red dots are weights in the input space, while the black lines are the connections between weights in the topology of the network. In plots \ref {subfig:som-one-before} and \ref {subfig:som-two-before} are shown the initial weights, representing also the arrangement of the weights matrix. Remember that in general, the displacement in the input space is different from the displacement in weight's matrix. In figures \ref {subfig:som-one-after} and \ref {subfig:som-two-after} show the displacement of the weights in the \textbf {input space} after the training procedure, that preserve the connections in the weights matrix.\relax }}{54}{figure.caption.176}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces This figure shows the pseudo-preference expressed by an auto-encoder trained on a $\mathcal {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }}{54}{figure.caption.181}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces This figure shows the pseudo-preference expressed by two SOMs, one with a linear topology (fig \ref {subfig:linear_som_res}), the other with a square topology (fig \ref {subfig:square_som_res}). Both have been trained on a $\mathcal {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }}{55}{figure.caption.183}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces List of all the 2D datasets used in the tests. Three of them contains circles, five contains lines and one contains parables.\relax }}{57}{figure.caption.186}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces AE$_1$ AUCs on circle3.\relax }}{75}{figure.caption.215}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces AE$_1$ AUCs on circle4.\relax }}{76}{figure.caption.216}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces AE$_1$ AUCs on circle5.\relax }}{76}{figure.caption.217}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces AE$_1$ AUCs on stair3.\relax }}{77}{figure.caption.218}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces AE$_1$ AUCs on stair4.\relax }}{77}{figure.caption.219}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces AE$_1$ AUCs on star5.\relax }}{78}{figure.caption.220}%
\contentsline {figure}{\numberline {A.7}{\ignorespaces AE$_1$ AUCs on star11.\relax }}{78}{figure.caption.221}%
\contentsline {figure}{\numberline {A.8}{\ignorespaces AE$_1$ AUCs on parables.\relax }}{79}{figure.caption.222}%
\contentsline {figure}{\numberline {A.9}{\ignorespaces AE$_1$ AUCs on lines.\relax }}{79}{figure.caption.223}%
\contentsline {figure}{\numberline {A.10}{\ignorespaces AE$_2$ AUCs on circle3.\relax }}{80}{figure.caption.224}%
\contentsline {figure}{\numberline {A.11}{\ignorespaces AE$_2$ AUCs on circle4.\relax }}{81}{figure.caption.225}%
\contentsline {figure}{\numberline {A.12}{\ignorespaces AE$_2$ AUCs on circle5.\relax }}{81}{figure.caption.226}%
\contentsline {figure}{\numberline {A.13}{\ignorespaces AE$_2$ AUCs on stair3.\relax }}{82}{figure.caption.227}%
\contentsline {figure}{\numberline {A.14}{\ignorespaces AE$_2$ AUCs on stair4.\relax }}{82}{figure.caption.228}%
\contentsline {figure}{\numberline {A.15}{\ignorespaces AE$_2$ AUCs on star5.\relax }}{83}{figure.caption.229}%
\contentsline {figure}{\numberline {A.16}{\ignorespaces AE$_2$ AUCs on star11.\relax }}{83}{figure.caption.230}%
\contentsline {figure}{\numberline {A.17}{\ignorespaces AE$_2$ AUCs on parables.\relax }}{84}{figure.caption.231}%
\contentsline {figure}{\numberline {A.18}{\ignorespaces AE$_2$ AUCs on lines.\relax }}{84}{figure.caption.232}%
\contentsline {figure}{\numberline {A.19}{\ignorespaces AE$_3$ AUCs on plane.\relax }}{85}{figure.caption.233}%
\contentsline {figure}{\numberline {A.20}{\ignorespaces AE$_3$ AUCs on paraboloid.\relax }}{86}{figure.caption.234}%
\contentsline {figure}{\numberline {A.21}{\ignorespaces AE$_3$ AUCs on sphere.\relax }}{86}{figure.caption.235}%
\contentsline {figure}{\numberline {A.22}{\ignorespaces AE$_4$ AUCs on plane.\relax }}{87}{figure.caption.236}%
\contentsline {figure}{\numberline {A.23}{\ignorespaces AE$_4$ AUCs on paraboloid.\relax }}{88}{figure.caption.237}%
\contentsline {figure}{\numberline {A.24}{\ignorespaces AE$_4$ AUCs on sphere.\relax }}{88}{figure.caption.238}%
\contentsline {figure}{\numberline {A.25}{\ignorespaces SOM$_1$ AUCs on circle3.\relax }}{89}{figure.caption.239}%
\contentsline {figure}{\numberline {A.26}{\ignorespaces SOM$_1$ AUCs on circle4.\relax }}{90}{figure.caption.240}%
\contentsline {figure}{\numberline {A.27}{\ignorespaces SOM$_1$ AUCs on circle5.\relax }}{90}{figure.caption.241}%
\contentsline {figure}{\numberline {A.28}{\ignorespaces SOM$_1$ AUCs on stair3.\relax }}{91}{figure.caption.242}%
\contentsline {figure}{\numberline {A.29}{\ignorespaces SOM$_1$ AUCs on stair4.\relax }}{91}{figure.caption.243}%
\contentsline {figure}{\numberline {A.30}{\ignorespaces SOM$_1$ AUCs on star5.\relax }}{92}{figure.caption.244}%
\contentsline {figure}{\numberline {A.31}{\ignorespaces SOM$_1$ AUCs on star11.\relax }}{92}{figure.caption.245}%
\contentsline {figure}{\numberline {A.32}{\ignorespaces SOM$_1$ AUCs on parables.\relax }}{93}{figure.caption.246}%
\contentsline {figure}{\numberline {A.33}{\ignorespaces SOM$_1$ AUCs on lines.\relax }}{93}{figure.caption.247}%
\contentsline {figure}{\numberline {A.34}{\ignorespaces SOM$_1$ AUCs on plane.\relax }}{94}{figure.caption.248}%
\contentsline {figure}{\numberline {A.35}{\ignorespaces SOM$_1$ AUCs on paraboloid.\relax }}{95}{figure.caption.249}%
\contentsline {figure}{\numberline {A.36}{\ignorespaces SOM$_1$ AUCs on sphere.\relax }}{95}{figure.caption.250}%
\contentsline {figure}{\numberline {A.37}{\ignorespaces SOM$_2$ AUCs on circle3.\relax }}{96}{figure.caption.251}%
\contentsline {figure}{\numberline {A.38}{\ignorespaces SOM$_2$ AUCs on circle4.\relax }}{97}{figure.caption.252}%
\contentsline {figure}{\numberline {A.39}{\ignorespaces SOM$_2$ AUCs on circle5.\relax }}{97}{figure.caption.253}%
\contentsline {figure}{\numberline {A.40}{\ignorespaces SOM$_2$ AUCs on stair3.\relax }}{98}{figure.caption.254}%
\contentsline {figure}{\numberline {A.41}{\ignorespaces SOM$_2$ AUCs on stair4.\relax }}{98}{figure.caption.255}%
\contentsline {figure}{\numberline {A.42}{\ignorespaces SOM$_2$ AUCs on star5.\relax }}{99}{figure.caption.256}%
\contentsline {figure}{\numberline {A.43}{\ignorespaces SOM$_2$ AUCs on star11.\relax }}{99}{figure.caption.257}%
\contentsline {figure}{\numberline {A.44}{\ignorespaces SOM$_2$ AUCs on parables.\relax }}{100}{figure.caption.258}%
\contentsline {figure}{\numberline {A.45}{\ignorespaces SOM$_2$ AUCs on lines.\relax }}{100}{figure.caption.259}%
\contentsline {figure}{\numberline {A.46}{\ignorespaces SOM$_2$ AUCs on plane.\relax }}{101}{figure.caption.260}%
\contentsline {figure}{\numberline {A.47}{\ignorespaces SOM$_2$ AUCs on paraboloid.\relax }}{102}{figure.caption.261}%
\contentsline {figure}{\numberline {A.48}{\ignorespaces SOM$_2$ AUCs on sphere.\relax }}{102}{figure.caption.262}%
\contentsline {figure}{\numberline {A.49}{\ignorespaces Comparison between models on circle3.\relax }}{103}{figure.caption.263}%
\contentsline {figure}{\numberline {A.50}{\ignorespaces Comparison between models on circle4.\relax }}{104}{figure.caption.264}%
\contentsline {figure}{\numberline {A.51}{\ignorespaces Comparison between models on circle5.\relax }}{104}{figure.caption.265}%
\contentsline {figure}{\numberline {A.52}{\ignorespaces Comparison between models on stair3.\relax }}{105}{figure.caption.266}%
\contentsline {figure}{\numberline {A.53}{\ignorespaces Comparison between models on stair4.\relax }}{105}{figure.caption.267}%
\contentsline {figure}{\numberline {A.54}{\ignorespaces Comparison between models on star5.\relax }}{106}{figure.caption.268}%
\contentsline {figure}{\numberline {A.55}{\ignorespaces Comparison between models on star11.\relax }}{106}{figure.caption.269}%
\contentsline {figure}{\numberline {A.56}{\ignorespaces Comparison between models on parables.\relax }}{107}{figure.caption.270}%
\contentsline {figure}{\numberline {A.57}{\ignorespaces Comparison between models on lines.\relax }}{107}{figure.caption.271}%
