\chapter{Proposed Method}
\label{ch:proposed_solution}

\paragraph{}
First, we must conceptually distinguish between two concepts: the models that produce the data and the estimated machine learning models. \textbf{Pattern} refers to the actual model that specifies the manifold on which the data is located; it can represent geometrical figures, such as lines, parables, and circles, as well as more intricate curves; rather, we will refer to \textbf{models} as those that are built from data using statistical and machine learning techniques.

\paragraph{}
The novel \textbf{model-based} algorithm presented in this thesis aims to address the shortcomings of current approaches, in particular extending PIF applying a reconstruction-based ensemble. In the case of structured anomalies the density-based and distance-based approaches do not work, because those concepts are uninformative. In the model-based approach, instead, PIF represents an interesting approach since uses the preference trick to embed the dataset into a new space and applies a kind of Isolation Forest on this new space. \newline
The preference trick, as explained in \ref{subsec:preference_embedding}, is a measure of the deviation of the point from the learned model. Thus, the trick is to generate a new space in which the more models have high preference (low deviation) for a point, the more that point is probably a normal point.

\paragraph{}
This approach is notable, but it assumes that the patterns in the dataset have a known shape and the algorithm cannot generalize if the pattern can not be, or it is too complex to be, expressed analytically. In order to build the preference representation, PIF samples a high number of models that try to express preferences towards normal points.  Therefore, if we use a model representing a line to search for the pattern, we will discover only patterns that are represented as a line. Yet, if we use circles we will find only patterns with a circular shape. 

\paragraph{}
Our approach tries to build more general models learned directly from data, which can take any shape; in this way, the model adapts to the shapes found in the dataset and do not forces the patterns to be as assumed. Is a similar, but actually different, solution to PIF, but in our approach the models automatically adapts depending on the manifolds in the dataset.

\section{Problem Formulation}
\label{sec:problem_formulation}

\paragraph{}
The \textbf{\textit{input}} data is a dataset $\mathcal{D}$ made of noisy points in a \textit{m-dimensional} space: $\mathcal{D} = \{ \textbf{p} + \eta | \textbf{p} \in \mathbb{R}^m \}$, where $\eta = [\eta_1, .., \eta_i, ..., \eta_m]$ and $\eta_i \approx \mathcal{N}(0,\ \sigma^2_i)$ represents the noise affecting the data. The noise is modeled as an additive Gaussian noise with zero mean and unknown variance, applied to each dimension of the input vector.

\paragraph{}
Data points are divided in two disjoint subsets, the \textbf{\textit{normal}} points and the \textbf{\textit{anomalous}} points: $\mathcal{D} = \mathcal{N} \cup \mathcal{A}$ with $\mathcal{A} \cap \mathcal{N} = \emptyset$, where $\mathcal{N}$ is the set of normal points and $\mathcal{A}$ is the set of anomalies. \newline
\textbf{Normal} points lies on a set of \textit{k} \textbf{manifolds} in the \textit{m-dimensional} space, $\mathcal{N} \subseteq \mathcal{M}_1 \cup ... \cup \mathcal{M}_k$; each manifold represents a generic parametric function $\mathcal{F}_{\theta_i}$, where $\theta_i$ represents the parameter vector of the \textit{i-th} manifold. In a noisy-free setup all normal points lie on the manifolds, so each point \textit{p} belonging to the \textit{i-th} manifold, must satisfy $\mathcal{F}_{\theta_i}(\textbf{p}) = 0$. Note that the \textit{i-th} manifold is represented by $\mathcal{F}_{\theta_i}(\textbf{p})$ and so it is composed as $\mathcal{M}_i = \{ \textbf{p} : \mathcal{F}_{\theta_i}(\textbf{p}) = 0 \}$. Since in our case the points are noisy, we can expect $\mathcal{F}_{\theta_i}(\textbf{p}) \approx 0\ \forall\ \textbf{p} \in \mathcal{M}_i$.  \newline
\textbf{Anomalous} points $\mathcal{A}$, instead, do not have a standard behaviour and so do not lie in any manifold; thus, they form a subset $\mathcal{A} \subset \mathcal{D}$ and we know that $\mathcal{A} \not\subseteq \mathcal{M}_1 \cup ... \cup \mathcal{M}_k$. An anomaly is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism.

\paragraph{}
The \textbf{\textit{output}} of the algorithm $\mathcal{S} = \{s(\textbf{p}) \in [0, 1]\ |\ \textbf{p} \in \mathcal{D}\}$ is an anomaly score for each point, $s:\ \mathcal{D} \to \mathbb{R}$, representing a sort of "probability" that the point is anomalous. Thus, the score of a point is higher for points that are more probable to be anomalous and lower for normal points: $s(a) \gg s(n)$ for all $a \in \mathcal{A}$ and $n \in \mathcal{N}$.
\newline
This task is quite challenging, since the total number of manifolds, their structure and the noise affecting the data are unknown. It is necessary to learn the parameters that govern the patterns directly from data, contaminated by the anomalies.

\section{Rationale}

\paragraph{}
Considering the problem of Anomaly Detection in a structured dataset, the approach presented in PIF represents a big change. Thanks to the preference trick we are able to embed points in a new space, representing how much the learned models express a preference for each point. \newline
The preference expressed by a model $\mathcal{G}_j$ for a point $\textbf{p}_i \in \mathcal{D}$ represents the inverse of the deviation of the point from the model. In practice, the more the point is near to its representation built by the model, the more the preference $[\textbf{pr}_i]_j$ is high. Therefore, the preference space reflects how much each point is fitted by each model. \newline
If we would know the true pattern that generated normal points, all normal points would have a very high preference - because the deviation from the pattern is relatively small - and all the anomalies would have a very low preference - because the deviation is higher.

\paragraph{}
Note that in the following we will use the terms \textit{preference space} and \textit{preference matrix} as synonyms, but in reality they are different: the preference \textit{matrix} refers to a particular pool of models, while the preference \textit{space} is the space in which data points are projected. As an example, if we build two different pools of models the preference matrices will be different, but the space in which the points are projected is the same.

\paragraph{}
The preference space allows us to isolate data points depending on how many models have expressed a high preference for the points, thus it is mandatory to be able to build a preference space that reflects the nature of the dataset. In order to do that, we need to build models that can understand which patterns governs the data and replicate them. \newline
This is where the \textbf{novelty} of the approach developed in this thesis comes in: we build a pool of non-linear Neural Networks that are able to extract meaningful and appropriate patterns from the dataset without any prior knowledge on the nature of this patterns, as opposed to the way it is done in PIF, in which we assume a specific pattern to search and use that model to build the preference matrix. We have studied two kinds of Neural Networks, the Auto-Encoders \cite{ae_paper} and the Self-Organizing-Maps \cite{som_paper}. Both this models are able to adapt their weights to create a \textit{lower-dimensional} representation of the dataset, being able to hold only important features; since the anomalies represent data outside the patterns, the Neural Networks are able to discard their information and rely only on normal points to build the pattern representation.

\paragraph{}
PIF assumes that the type of patterns found in the dataset is known; e.g., it is known that normal data are originated from a line or from a circle; consequently, PIF will use, respectively, a line and a circle as learners and therefore the preference matrix will depend on the preferences expressed by this models. \newline
Such assumption reduces the use cases of PIF, because especially in real-world cases it is rare that the normal behavior is expressed by such simple models. \newline
Our algorithm, instead, does not have that limiting assumptions, but exploits the fact that several little models are built and try to approximate the patterns using those models. We want to build more general models learned directly from data, which can take any shape; in this way, the model adapts to the shapes found in the dataset and do not forces the patterns to be as assumed. Is a similar, but actually different, solution to PIF, because in our approach the models automatically adapts depending on the manifolds in the dataset.

\paragraph{}
The \textbf{goal of this thesis} is to show that using an ensemble of non-linear models such as Neural networks, trained on a subset of the dataset, we are able to approximate the patterns. \newline
We build models of lower degree with respect to the patterns' degree that try to approximate the manifold locally. At first, we will use as models Linear Regression and very simple Neural Networks with Auto-Encoder structure, but then we will also explore more complex and different models, such as deeper and wider Auto-Encoders and Self Organizing Maps.

\section{Algorithm}
\label{sec:algorithm}
The algorithm we propose for Anomaly Detection in structured normal data contaminated with high percentage of anomalies is structured similarly as PIF algorithm \ref{alg:pif}. It is composed of four main phases, that are listed below. The algorithm is shown in \ref{alg:npif}.

\begin{itemize}
    \item \textbf{Model building}: (lines 1-3) build the pool of models and train each model on a subset of the dataset, $\mathcal{MSS} \subset \mathcal{D}$, called \textbf{Minimum Sample Set} and has \textbf{cardinality} $\rho$, i.e. $|\mathcal{MSS}| = \rho$. This subset has the same role as in PIF, with the difference that in this case it represents the data instances on which train each model, while in PIF it represents the minimum samples needed to instantiate a model, e.g. two or three points are needed to instantiate respectively a line or a circle.
    
    \item \textbf{Preference Embedding}: (lines 5-18) embed each point into the preference space, generating the preference matrix $P \in \mathbb{R}^{N \times k}$ with $|\mathcal{D}| = N$ and $k$ the number of models. The value of element $(i, j)$ depends on the preference expressed by model $\mathcal{G}_j$ for point $\textbf{p}_i$, noted as $\delta_{ij}$.
    
    \item \textbf{Isolation Voronoi Forest}: (lines 20-21) apply the iVor (\ref{alg:piforest}) on the generated preference matrix, building the isolation forest made of Pi-Trees (\ref{alg:pitree}) on which compute the score.
    
    \item \textbf{Score computation}: (lines 23-30) compute the path length of each instance in each tree and finally compute the score, averaging between the computed path lengths and scaling by $c(\psi)$, refer to \ref{eq:ifor_score_ad}.
    
\end{itemize}

\begin{algorithm}[h!]
    \caption{\textit{Neural-PIF}}
    \label{alg:npif}
    \textbf{Input:} $\mathcal{D}$ - dataset, $k$ - number of models, $\rho$ - $\mathcal{MSS}$ size, $\tau$ - threshold,  $t$ - number of trees, $\psi$ - sub-sampling size, $b$ - branching factor \\
    \textbf{Output:} Anomaly scores $\{\ s_\psi(\mathcal{E}(\textbf{p}_i))\ \}_{i=1, ..., n}$
    \begin{algorithmic}[1]
        \State\textit{/* Build models */}
        \State $\mathcal{MSS} \in \mathbb{R}^{N \times \rho} \gets$ \textit{uniformSampling}($\mathcal{D}$, $\rho$)
        \Comment{Sample the Minimum Sample Sets, one for each instance $n \in N$, of size $\rho$}
        \State $\mathcal{M}_k \gets buildModels(\mathcal{D}, k, \rho)$
        \State
        \State \textit{/* Preference embedding $\mathcal{E}$ */}
        % \State $P \gets preferenceEmbedding(\mathcal{D},\ \mathcal{M}_k, \tau) \in \mathbb{R}^{nk}$
        \State $P \in \mathbb{R}^{nk}$ \Comment{initialize matrix}
        \ForAll{$i \in [0, n]$}
            \State $\textbf{p}_i \gets $ \textit{i-th} point of dataset
            \ForAll{$j \in [0, k]$}
                \State $\hat{\textbf{p}}_j \gets$ prediction of \textit{j-th} model for $\textbf{p}_i$
                \State $\delta_{ij} \gets$ distance between $\textbf{p}_i$ and $\hat{\textbf{p}}_i$
                \If{$\delta_{ij} \leq \tau$}
                    \State $P_{ij} \gets exp(-\delta_{ij} / \tau)$
                \Else
                    \State $P_{ij} \gets 0$
                \EndIf
            \EndFor
        \EndFor
        \State
        \State \textit{/* Train Preference Isolation Forest */}
        \State $F \gets \textit{PI-Forest}(P,\ t,\ \psi,\ b)$
        \State
        \State \textit{/* Scoring input data */}
        % \State $S_\psi(\mathcal{D}) \gets scoreDataset(P, F)\}$
        \ForAll{$i \in [1,\ |P|]$}
            \State $\textbf{h} \gets [0,\ ...,\ 0] \in \mathbb{R}^t$
            \ForAll{$j \in [1,\ t]$}
                \State $T \gets $ \textit{j-th} PI-Tree in $F$
                \State $[\textbf{h}]_j \gets \textit{pathLength}(\textbf{p}_i, T, 0)$
            \EndFor
            \State $s_\psi(\textbf{p}_i) \gets 2^{- \frac{E(\textbf{h}(\textbf{p}_i))}{c(\psi)}}$
        \EndFor
        \State \Return $s_\psi(\textbf{p}_i)_{i=1,\ ...,\ n}$
    \end{algorithmic}
\end{algorithm}

\section{Pattern learners}

In this section we will show different learners, that present different properties and capabilities in extracting the pattern from data. We will start with the most simple analytical models, like the Line and the Plane, showing that theoretically with an infinite number of those models it is possible to approximate a curve or a surface, respectively. \newline
Next, we will how it is possible to extract more complex patterns by employing more general models, like Auto-Encoders and Self-Organizing Maps.

\subsection{Line and Plane}
\paragraph{}
The first model from which to start is the most simple one, a straight line. It is known \cite{tangent_line_approx} that it is possible to linearly approximate all kind of curves at each point thanks to the tangent passing for that point. \newline
Given a function $y = f(x)$ that is differentiable at a point $p$, the function $f(\cdot)$ is locally linear and at the point $(p, f(p))$ can be approximated by its tangent. Thus, we can approximate the original function $f(\cdot)$ with a simpler function $L(\cdot)$ that is linear. We need to recall that when $f(x)$ is differentiable at $x = p$, the value of $f'(p)$ provides the slope of the tangent line to $y = f(x)$ at the point $(p, f(p))$. Therefore, the tangent line will have an explicit form like 
\begin{figure}[ht]
    \label{fig:linear_approx}
    \centering
    \includesvg[width=0.7\textwidth]{Images/problem_solution/linear_approx.svg}
    \caption{Example of a linear approximation on a generic function $y = f(x)$. In blue the true function, in green the tangent at the point $(p, f(p))$ that is marked in red.}
\end{figure}
\begin{equation}
    L(x) = f'(p)(x-p) + f(p)
\end{equation}
We will call $L(x)$ the \textit{first degree local linearization} of $f(x)$ at the point $(p, f(p))$ i.e. $L(x) \approx f(x)$ for $x \approx p$.

\paragraph{}
At this point we know that we can locally approximate any given function $f(\cdot)$ with a line, at each point; thus, ideally, with an infinite number of lines we can also derive the shape of the function. Starting from this idea, we would like to approximate any manifold with local approximations. In a \textit{two-dimensional} case, the linear approximation can be thought as a line; in a \textit{three-dimensional} case, the linear approximation is a plane.

\paragraph{}
In fact, considering a plane containing the point $\textbf{p} = (x_0, y_0, z_0)$ with a normal vector $\textbf{n} = \begin{bmatrix} a, & b, & c, \end{bmatrix}$, its equation is given by
\begin{equation}
    a(x - x_0) + b(y - y_0) + c(z - z_0) = 0.
\end{equation}
We know \cite{plane_local_approx} also that the gradient vector $\nabla f(x_0, y_0)$ is perpendicular to the level curve $f(x, y) = k$ at the point $\textbf{p}$; likewise, the gradient vector $\nabla f(x_0, y_0, z_0)$ is perpendicular to the level surface $f(x, y, z) = k$ at the point $\textbf{p}$. 

We should recall that the gradient vector is $\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x}, & \frac{\partial f}{\partial y}, & \frac{\partial f}{\partial z} \end{bmatrix}$. Therefore, the tangent plane to the surface given by $f(x, y, z) = k$ at the point $\textbf{p}$, has the equation
\begin{equation}
    \frac{\partial f(x_0, y_0, z_0)}{\partial x}(x - x_0) + \frac{\partial f(x_0, y_0, z_0)}{\partial y}(y - y_0) + \frac{\partial f(x_0, y_0, z_0)}{\partial z}(z - z_0) = 0
\end{equation}

If we consider now a generic function of the form $z = f(x, y)$ and $F(x, y, z) = f(x, y) - z$ we can see that the surface given by $z = f(x, y)$ is identical to the surface given by $F(x, y, z) = 0$. The gradient of this function will be $\nabla F = \begin{bmatrix} \frac{\partial F}{\partial x}, & \frac{\partial F}{\partial y} & -1\end{bmatrix}$, because the last element is computed as $\frac{\partial }{\partial z}(f(x, y) - z)$. \newline
From the equation of the plane obtained above and solving for $z$, we obtain the local 2D approximation of a 3D surface:
\begin{equation}
    z = \frac{\partial f(x_0, y_0)}{\partial x}(x - x_0) + \frac{\partial f(x_0, y_0)}{\partial y}(y - y_0) + f(x_0, y_0)
\end{equation}

\begin{figure}[htb]
    \centering
    \includesvg[width=0.5\textwidth]{Images/problem_solution/local_plane.svg}
    \caption{Example of a plane, $\alpha$, the is tangent to the surface $f(x, y, z)$ at the point $\textbf{p}$.}
    \label{fig:local_plane}
\end{figure}

Therefore, our idea is to use the model line (or plane for the \textit{three-dimensional} space) not as the known pattern as in PIF, but as the model with which we can approximate any manifold. The algorithm using this model will function in the same way as in PIF when the pattern is a set of lines, but we show 
this procedure in this context in order to have a baseline to the next models, that should be able to locally approximate more complex functions and to better incorporate the patterns.

\subsection{Auto-encoders}
Auto-encoders (\ref{subsec:ae}) are a particular type of neural network whose structure consists of two parts: an encoder and a decoder. \newline
The \textit{encoder} tries to encode the input data, by reducing its dimensionality and creating the \textit{latent representation}. The \textit{decoder}, on the other hand, receives the latent representation and tries to reconstruct the original input. The trick that allows the network to \textbf{not} learn the identity matrix, which would be a copy of the original dataset, comes from the fact that the latent representation has fewer dimensions than the original input, thus it is forced to learn a representation of the input different from the identity. \newline
The output of the network and the training performed varies a lot depending on this factors:
\begin{enumerate}
    \item The \textbf{depth} of the network;
    \item The \textbf{width} of the network;
    \item The \textbf{activation} function of the layers. The major difference is between linear and non-linear activation functions: the latter allow the latent-space to be a non-linear transformation of the original input space and so allows to better capture the structure of the dataset.
\end{enumerate}

\begin{figure}[tb]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/autoencoders/dataset.svg}
         \caption{The dataset}
         \label{subfig:ds}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/autoencoders/predictions_ae2.svg}
         \caption{MLP}
         \label{subfig:mlp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includesvg[width=\textwidth]{Images/problem_solution/autoencoders/predictions_ae1.svg}
         \caption{Auto-Encoder}
         \label{subfig:ae1}
     \end{subfigure}
     \caption{This figure shows the utility of the constraint on the latent representation's number of dimensions. In image \ref{subfig:mlp}, a Multi Layer Perceptron (MLP) network has learned to reconstruct the input vector generating a new vector in the same space; the problem is that this network didn't generate any latent representation with fewer dimension, thus it has learnt the identity matrix. On the contrary, in image \ref{subfig:ae1}, an Auto-Encoder with a latent representation with less dimensions than the input space has learned a new representation of the input dataset, not the identity matrix.}
     \label{fig:ae_constraint}
\end{figure}

Usually, as explained in \cite{ae_anom_detect}, auto-encoders for anomaly detection are trained on a dataset of normal instances that allow the network to understand the underlying pattern, being able to incorporate the patterns present in the normal instances. In the evaluation stage, the one in which an anomaly score is given to the instances, the score depends on the reconstruction error made by the network: the higher the error, the more probable the instance is anomalous. Indeed, since the network has incorporated the normal pattern, when the input is anomalous the reconstructed output will be very different from the input, while when the input is a normal point, the output of the network will be very similar to the original input. 

\paragraph{}
This method produces very good results, but it highly depends on the dataset used for training. In fact, it needs a human-labelled dataset in which normal points are separated from anomalies and the part of the dataset containing normal points should take in account all the possible behaviours that can be defined "normal"; if the set of normal points contains contaminations, the network will learn also the anomalous behaviour and if it is not highly representative of the normal behaviour, the model will produce many false negatives. \newline
The advantage is that it can be used to label single instances as normal or anomalous; indeed, once the network is trained it is possible to use it for any other instance of the same kind as the training data.

\paragraph{}
Our algorithm, instead, is able to train the pool of models directly on the \textbf{unlabelled} dataset, without the need for a separation between normal points and anomalies. Training the auto-encoders on a subset of the dataset allows each model to learn the pattern of a portion of the data and extract from it the normal behaviour. Using the preference trick, each model assigns a preference for its own "normal" instances using the learnt representation; in this way, we have several experts of some portion of the dataset and with Pi-Forest the algorithm can understand which instances are normal and which are not.

\paragraph{}
In the following sections we will show that an auto-encoder trained on a pure dataset, i.e. that does not contain anomalies, is able to learn the equation of a line or of a plane and the relationship between network's weights and coefficients of the pattern. \newline
This is crucial for our purposes, because it means that the auto-encoder is actually able to learn a useful representation of the pattern encoded, both in the encoder and in the decoder. Obviously this are examples showing the case in which the dataset is pure and the pattern is rather simple, but they nonetheless demonstrate the possibilities of an auto-encoder.

\subsubsection{Relationship between auto-encoders and lines}
\paragraph{}
Consider an auto-encoder with two input neurons, two output neurons, one hidden layer comprising one neuron and having the identity function $f(x) = x$ as activation. It will have four weights ($w_{02}, w_{12}, w_{23}, w_{24}$) and three biases ($b_{2}, b_{3}, b_{4}$).
\begin{figure}[ht]
    \centering
    \includesvg[width=0.7\textwidth]{Images/problem_solution/autoencoders/ae_212.svg}
    \caption{Structure of the network used in this section. The pedices of the weights are the starting neuron's index and the connected neuron's index. So, for example, weight $w_{02}$ goes from neuron $n_0$ to neuron $n_2$, or weight $w_{23}$ goes from neuron $n_2$ to neuron $n_3$. The biases have the same index to which they belong; for example, bias $b_4$ is the bias of neuron $n_4$.}
    \label{fig:ae_212}
\end{figure}
As we know, the output of a neuron is computed as the dot product of the weights and the input vector. Thus, the output of the hidden neuron can be computed as follows:
\begin{equation}
    h = w_{02} x + w_{12} y + b_2
\end{equation}
Therefore, the output of the network, $\hat{x}, \hat{y}$ is:
\begin{equation}
\label{eq:x_hat}
    \hat{x} = w_{23} \cdot h + b_{3}
\end{equation}
\begin{equation}
\label{eq:y_hat}
    \hat{y} = w_{24} \cdot h + b_{4}
\end{equation}
The goal of the network is to encode the input vector computing its latent representation and then decode the latent representation to reconstruct the input vector. It is trained to minimize the MSE (equation \ref{eq:MSE}), thus the error is computed calculating the euclidean distance between the prediction and the real point. Thus, the network must incorporate the data on which is trained to be able to replicate the same structure. \newline
Training this kind of network on a pure dataset in which the pattern is a line, we will show that exists a relationship between the weights and biases of the network and the coefficients of the pattern. Indeed, it is possible to compute those coefficients using the weights of the decoder part of the network.

\paragraph{}
From \cite{line_between_two_points} we know that given two points, $P_1 = \begin{bmatrix}x_1 & y_1\end{bmatrix}$ and $P_2 = \begin{bmatrix}x_2 & y_2\end{bmatrix}$, the line passing the two points is equal to the vector $P_2P_1$: 
$v_r = 
\begin{bmatrix}
    x_2 - x_1 \\
    y_2 - y_1
\end{bmatrix}$. Given the vector, we can write the vectorial equation of a line:
\begin{equation}
    \begin{bmatrix}
        x \\
        y
    \end{bmatrix}
    =
    \begin{bmatrix}
        x_1 \\
        y_1
    \end{bmatrix}
    + t \cdot \begin{bmatrix}
        x_2 - x_1 \\
        y_2 - y_1
    \end{bmatrix}
\end{equation}
Considering now a generic point $P$ belonging to the line, this forms a vector with respect to a point of the line, e.g. $P_1$:
\begin{equation}
    \overrightarrow{\rm P_1P} = P - P_1 = \begin{bmatrix}
        x - x_1 \\
        y - y_1
    \end{bmatrix}
\end{equation}
The vector $P_1P$ must be parallel and proportional to the vector $P_1P_2$, because it lies on the line equivalent to the vector $v_r$; thus, the vectors $P_1P$ and $P_1P_2$ must be linearly dependent and the equation of the line can be obtained solving the equation
\begin{equation}
    det \left ( 
    \begin{bmatrix}
        x - x_1 & x_2 - x_1 \\
        y - y_1 & y_2 - y_1
    \end{bmatrix}
    \right ) = 0
\end{equation}
that can be solved as follows:
\begin{equation}
    \begin{gathered}
        (x-x_1)(y_2-y_1) - (y-y_1)(x_2-x_1) = 0 \\
        xy_2 - x_1y_2 + x_1y_1 - yx_2 + yx_1 + y_1x_2 - y_1x_1 = 0 \\
        x_1y - x_2y - xy_1 + x_2y_1 + xy_2 - x_1y_2 = 0 \\
        y(x_1-x_2) + x(y_2 - y_1) -x_1y_2 + x_2y_1 = 0 \\
        y(x_1-x_2) = -(y_2-y_1)x + x_1y_2 - x_2y_1 \\
        y(x_1-x_2) = (y_1-y_2)x + x_1y_2 - x_2y_1 \\
        y = \frac{y_1-y_2}{x_1-x_2}x + \frac{x_1y_2 - x_2y_1}{x_1-x_2} \\
    \end{gathered}
\end{equation}
From this, we know that the slope of the line is $m = \frac{y_1-y_2}{x_1-x_2}$ and the intercept is $q = \frac{x_1y_2 - x_2y_1}{x_1-x_2}$.

\paragraph{}
Given two points $\hat{P}_1$ and $\hat{P}_2$ predicted by the network and using equations \ref{eq:x_hat} and \ref{eq:y_hat}, we can write the following:
\begin{equation}
    \begin{gathered}
        \hat{P_1} = \begin{bmatrix}
            \hat{x}_1 \\
            \hat{y}_1
        \end{bmatrix}
        = \begin{bmatrix}
            w_{23}h_1 + b_{3} \\
            w_{24}h_1 + b_{4}
        \end{bmatrix} \\
        \hat{P_2} = \begin{bmatrix}
            \hat{x}_2 \\
            \hat{y}_2
        \end{bmatrix}
        = \begin{bmatrix}
            w_{23}h_2 + b_{3} \\
            w_{24}h_2 + b_{4}
        \end{bmatrix}
    \end{gathered}
\end{equation}
where $h_1$ and $h_2$ are the hidden representations for the first and second point, respectively.
Substituting this values in the slope formula, we obtain the following result:
\begin{equation}
    \begin{gathered}
        m = \frac{\hat{y}_1 - \hat{y}_2}{\hat{x}_1 - \hat{x}_2} \\
        m = \frac{w_{24}h_1 + b_{4} - w_{24}h_2 - b_{4}}{w_{23}h_1 + b_{3} - w_{23}h_2 - b_{3}} \\
        m = \frac{w_{24}(h_1 - h_2)}{w_{23}(h_1 - h_2)} \\
        m = \frac{w_{24}}{w_{23}}
    \end{gathered}
\end{equation}
The same can be done with the equation of the intercept:
\begin{equation}
    \begin{gathered}
        q = \frac{\hat{x}_1\hat{y_2} - \hat{x}_2\hat{y}_1}{\hat{x}_1 - \hat{x}_2} \\
        q = \frac{(w_{23}h_1 + b_{3})(w_{24}h_2 + b_{4}) - (w_{23}h_2 + b_{3})(w_{24}h_1 + b_{4})}{w_{23}h_1 + b_{3} - w_{23}h_2 - b_{3}} \\
        q = \frac{w_{23}h_1b_{4} + w_{24}h_2b_{3} - w_{23}h_2b_{4} - w_{24}h_1b_{3}}{w_{23}(h_1 - h_2)} \\
        q = \frac{w_{24}b_{3}(h_2-h_1) + w_{23}b_{4}(h_1-h_2)}{w_{23}(h_1-h_2)} \\
        q = \frac{(h_1-h_2)(w_{23}b_{4} - w_{24}b_{3})}{w_{23}(h_1-h_2)} \\
        q = b_{4} - \frac{w_{24}}{w_{23}}b_{3}
    \end{gathered}
\end{equation}
At this point, we know that the network is able to encode the two input numbers in one number and then construct the equation of the line in the decoder. Indeed, we have shown that from the weights and the biases of the decoder is possible to reconstruct the equation of the line that generated the pattern.

\paragraph{}
One simple case that shows the correctness of this result is when the latent representation is equal to the $x$ coordinate of the input point. \newline
In this case, to compute the output for the $x$ coordinate is necessary to have $w_{23} = 1$ and $b_3 = 0$. Therefore, the slope of the line will be $m = \frac{w_{24}}{w_{23}} = \frac{w_{24}}{1} = w_{24}$ and the intercept will be $q = b_4 - 1 \cdot b_3 = b_4$. \newline
Thus, the $y$ coordinate of the input point is computed in the exact same way as is done when generating the pattern, because they are computed in the following way:
\begin{equation}
    \begin{gathered}
        y = m \cdot x + q \\
        \hat{y} = w_{24} \cdot h + b_4 = m \cdot x + q \quad if\ x = h\\
    \end{gathered}
\end{equation}

\subsubsection{Relationship between auto-encoders and planes}
\label{subsec:ae-planes}
\paragraph{}
Moving from the \textit{two-dimensional} to the \textit{three-dimensional} space, we want to show that the auto-encoder architecture is also able to incorporate the equation of a plane, that is the \textit{three-dimensional} "equivalent" of the line. \newline
In this case the network has a structure made of three input neurons, two neurons in the hidden layer and three neurons in the output layer. The \textit{three-dimensional} input vector is embedded in a \textit{two-dimensional} vector and then the original point will be reconstructed from the latent representation thanks to the decoder ability.

\paragraph{}
Similarly as we have done for the \textit{two-dimensional} case, we will define the network as follows: we have three input neurons, $n_0$, $n_1$, $n_2$ corresponding to the tree coordinate of an input point, two hidden neurons $n_3$ and $n_4$ and finally three output neurons, $n_5$, $n_6$, $n_7$, corresponding to the three coordinates of the predicted point.
\begin{figure}[ht]
    \centering
    \includesvg[width=0.7\textwidth]{Images/problem_solution/autoencoders/ae_323.svg}
    \caption{The structure of the network used in this section. The convention for naming the weights and biases is the same used in figure \ref{fig:ae_212}.}
    \label{fig:ae_323}
\end{figure}
Also in this case we can define the output of each neuron combining the weights entering the neuron and the biases. The hidden representation will be represented as a \textit{two-dimensional} vector, defined as follows:
\begin{equation}
    \begin{gathered}
        h = (h_3, h_4) \\
        h_3 = w_{03}x + w_{13}y + w_{23}z + b_3 \\
        h_4 = w_{04}x + w_{14}y + w_{24}z + b_4
    \end{gathered}
\end{equation}
The output values are instead computed as follows:
\begin{equation}
    \begin{gathered}
        \hat{x} = w_{35}h_3 + w_{45}h_4 + b_5 \\
        \hat{y} = w_{36}h_3 + w_{46}h_4 + b_6 \\
        \hat{z} = w_{37}h_3 + w_{47}h_4 + b_7
    \end{gathered}
\end{equation}
As already done for the line, we will show how it is possible to formulate the equation of a plane. At section \ref{sec:code_plane} there is the code with which we made the calculations to obtain the formulas. \newline
From \cite{plane_between_three_points} we know that the canonical equation of a plane is $ax + by + cz + d = 0$, where $a, b, c, d \in \mathbb{R}$ and $a, b, c$ are not simultaneously null. From Euclidean Geometry, we know that for three non-aligned points passes one and only one plane; thus, fixing an ortonormal cartesian reference system $RC(O, i, j, k)$, consider three non-aligned points:
\begin{equation}
    \begin{gathered}
        P_1 = (x_1, y_1, z_1) \\
        P_2 = (x_2, y_2, z_2) \\
        P_3 = (x_3, y_3, z_3)
    \end{gathered}
\end{equation}
Given that $P_1, P_2, P_3$ are not aligned, the vectors of each chosen couple, $\overrightarrow{P_1P_2}, \overrightarrow{P_1P_3}, \overrightarrow{P_2P_3}$ are linearly independent between them. If this is not the case, one couple will be linearly dependent and would be parallel, thus we would have three points lying on the same line. \newline
Considering a generic point $P = (x, y, z)$, will belong to the plane if and only if the vectors $\overrightarrow{P_1P}, \overrightarrow{P_1P_2}, \overrightarrow{P_1P_3}$ are coplanar. This is equivalent to ask that the determinant of the matrix having as rows the components of these vectors with respect to the base vector $(i, j, k)$ is null. \newline
We can rewrite the three vectors as following:
\begin{equation}
    \begin{gathered}
        \overrightarrow{P_1P} = P - P_1 = (x-x_1)i + (y-y_1)j + (z-z_1)k \\
        \overrightarrow{P_1P_2} = P_2 - P_1 = (x_2-x_1)i + (y_2-y_1)j + (z_2-z_1)k \\
        \overrightarrow{P_1P_3} = P_3 - P_1 = (x_3-x_1)i + (y_3-y_1)j + (z_3-z_1)k \\
    \end{gathered}
\end{equation}
Finally, we can say that the three vectors $\overrightarrow{P_1P}$, $\overrightarrow{P_1P_2}$ and $\overrightarrow{P_1P_3}$ are coplanars if and only if 
\begin{equation}
    det \left (\begin{bmatrix}
        x - x_1 & y - y_1 & z - z_1 \\
        x_2 - x_1 & y_2 - y_1 & z_2 - z_1 \\
        x_3 - x_1 & y_3 - y_1 & z_3 - z_1 \\
    \end{bmatrix} \right ) = 0
\end{equation}
Solving the equation with Laplace rules with respect to the first row, we get
\begin{equation}
    \label{eq:plane_estim}
    a(x - x_1) + b(y - y_1) + c(z - z_1) = 0
\end{equation}
where
\begin{equation}
    \label{eq:plane_coefficients}
    \begin{gathered}
        a = \ det \left (\begin{bmatrix}
            y_2 - y_1 & z_2 - z_1 \\
            y_3 - y_1 & z_3 - z_1 
        \end{bmatrix} \right ) \\
        b = - det \left (\begin{bmatrix}
            x_2 - x_1 & z_2 - z_1 \\
            x_3 - x_1 & z_3 - z_1
        \end{bmatrix} \right ) \\
        c = \ det \left (\begin{bmatrix}
            x_2 - x_1 & y_2 - y_1 \\
            x_3 - x_1 & y_3 - y_1
        \end{bmatrix} \right )
    \end{gathered}
\end{equation}
Rearranging equation \ref{eq:plane_estim} we get
\begin{equation}
    \begin{gathered}
        a(x - x_1) + b(y - y_1) + c(z - z_1) = 0 \\
        ax - ax_1 + by - by_1 + cz - cz_1 = 0 \\
        ax + by + cz - ax_1 - by_1 - cz_1 = 0
    \end{gathered}
\end{equation}
And with $d = -ax_1 - by_1 - cz_1$, we get the equation of the plane:
$ax + by + cz + d = 0$. From this, we can explicit the z-coordinate obtaining
\begin{equation}
    \label{eq:plane_z_explicit}
    z = -\frac{a}{c} x - \frac{b}{c} y - \frac{d}{c}
\end{equation}

\paragraph{}
At this point, we know how to compute the parameters of the equation of a plane given three points that are no coplanar. Our goal is to show that from the parameters of the neural network at figure \ref{fig:ae_323} it is possible to compute the equation of a plane, independent from the input point and the latent representation. \newline
As done in the case of the line, consider three points predicted from the network:
\begin{equation}
    \label{eq:ae_323_points}
    \begin{gathered}
        \hat{P_1} = \begin{bmatrix}
            \hat{x}_1 \\
            \hat{y}_1 \\
            \hat{z}_1
        \end{bmatrix}
        = \begin{bmatrix}
            w_{35}h_{3_1} + w_{45}h_{4_1} + b_{5} \\
            w_{36}h_{3_1} + w_{46}h_{4_1} + b_{6} \\
            w_{37}h_{3_1} + w_{47}h_{4_1} + b_{7} 
        \end{bmatrix} \\
        \hat{P_2} = \begin{bmatrix}
            \hat{x}_2 \\
            \hat{y}_2 \\
            \hat{z}_2
        \end{bmatrix}
        = \begin{bmatrix}
            w_{35}h_{3_2} + w_{45}h_{4_2} + b_{5} \\
            w_{36}h_{3_2} + w_{46}h_{4_2} + b_{6} \\
            w_{37}h_{3_2} + w_{47}h_{4_2} + b_{7} 
        \end{bmatrix} \\
        \hat{P_3} = \begin{bmatrix}
            \hat{x}_3 \\
            \hat{y}_3 \\
            \hat{z}_3
        \end{bmatrix}
        = \begin{bmatrix}
            w_{35}h_{3_3} + w_{45}h_{4_3} + b_{5} \\
            w_{36}h_{3_3} + w_{46}h_{4_3} + b_{6} \\
            w_{37}h_{3_3} + w_{47}h_{4_3} + b_{7} 
        \end{bmatrix}
    \end{gathered}
\end{equation}
Now, we can compute the coefficients of the plane substituting the three points in place of the generic coordinates, developing equation \ref{eq:plane_coefficients}:
\begin{equation}
    \begin{gathered}
        a = \hat{y}_{1} \hat{z}_{2} - \hat{y}_{1} \hat{z}_{3} - \hat{y}_{2} \hat{z}_{1} + \hat{y}_{2} \hat{z}_{3} + \hat{y}_{3} \hat{z}_{1} - \hat{y}_{3} \hat{z}_{2} \\
        b = - \hat{x}_{1} \hat{z}_{2} + \hat{x}_{1} \hat{z}_{3} + \hat{x}_{2} \hat{z}_{1} - \hat{x}_{2} \hat{z}_{3} - \hat{x}_{3} \hat{z}_{1} + \hat{x}_{3} \hat{z}_{2} \\
        c = \hat{x}_{1} \hat{y}_{2} - \hat{x}_{1} \hat{y}_{3} - \hat{x}_{2} \hat{y}_{1} + \hat{x}_{2} \hat{y}_{3} + \hat{x}_{3} \hat{y}_{1} - \hat{x}_{3} \hat{y}_{2} \\
        d = -a\hat{x}_1 - b\hat{y}_1 - c\hat{z}_1 = - \hat{x}_{1} \hat{y}_{2} \hat{z}_{3} + \hat{x}_{1} \hat{y}_{3} \hat{z}_{2} + \hat{x}_{2} \hat{y}_{1} \hat{z}_{3} - \hat{x}_{2} \hat{y}_{3} \hat{z}_{1} - \hat{x}_{3} \hat{y}_{1} \hat{z}_{2} + \hat{x}_{3} \hat{y}_{2} \hat{z}_{1}
    \end{gathered}
\end{equation}
For our calculations, we need also to compute the $-\frac{a}{c}$, $-\frac{b}{c}$ and $-\frac{d}{c}$ values, but we omit them for brevity.

\paragraph{}
The last step of our demonstration is to substitute the values of the various $\hat{x}$, $\hat{y}$ and $\hat{z}$ with the values of the points in \ref{eq:ae_323_points}. We get
\begin{equation}
    \begin{gathered}
        - \frac{a}{c} = \frac{- w_{36} w_{47} + w_{37} w_{46}}{w_{35} w_{46} - w_{36} w_{45}} \\
        - \frac{b}{c} = \frac{w_{35} w_{47} - w_{37} w_{45}}{w_{35} w_{46} - w_{36} w_{45}} \\
        - \frac{d}{c} = \frac{b_{5} w_{36} w_{47} - b_{5} w_{37} w_{46} - b_{6} w_{35} w_{47} + b_{6} w_{37} w_{45} + b_{7} w_{35} w_{46} - b_{7} w_{36} w_{45}}{w_{35} w_{46} - w_{36} w_{45}} = \\
        \quad\quad = b_7 - b_5 \left (\frac{- w_{36} w_{47} + w_{37} w_{46}}{w_{35} w_{46} - w_{36} w_{45}} \right ) - b_6 \left (\frac{w_{35} w_{47} - w_{37} w_{45}}{w_{35} w_{46} - w_{36} w_{45}} \right ) = \\
        \quad\quad = b_7 + \frac{a}{c} \cdot b_5 + \frac{b}{c} \cdot b_6
    \end{gathered}
\end{equation}

\paragraph{}
This equations show that an auto-encoder structured in this form is able to reconstruct a 3D point belonging to a plane from its 2D latent representation. In the decoder we have all the ingredients to incorporate the equation of the plane and to generate new points belonging to it. \newline
The entire network works in the training stage learns how to reproduce the input points, until it converges and the equation of the plane is encoded in the weights of the decoder part. At this point, we are able to generate 3D points belonging to the plane starting from 2D points generated e.g. from noise. 

\subsection{Self Organizing Maps}
\begin{figure}[hb]
     \centering
     \begin{subfigure}{0.4\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_line_before.svg}
         \caption{\textit{1D} before training}
         \label{subfig:som-one-before}
     \end{subfigure}
     \begin{subfigure}{0.4\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_line_after.svg}
         \caption{\textit{1D} after training}
         \label{subfig:som-one-after}
     \end{subfigure}
     \\
     \begin{subfigure}{0.4\textwidth}
         \centering
         \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_square_before.svg}
         \caption{\textit{2D} before training}
         \label{subfig:som-two-before}
     \end{subfigure}
     \begin{subfigure}{0.4\textwidth}
         \centering
         \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_square_after.svg}
         \caption{\textit{2D} after training}
         \label{subfig:som-two-after}
     \end{subfigure}
     \caption{Two examples of how SOM weights can be arranged. Red dots are weights in the input space, while the black lines are the connections between weights in the topology of the network. In plots \ref{subfig:som-one-before} and \ref{subfig:som-two-before} are shown the initial weights, representing also the arrangement of the weights matrix. Remember that in general, the displacement in the input space is different from the displacement in weight's matrix. In figures \ref{subfig:som-one-after} and \ref{subfig:som-two-after} show the displacement of the weights in the \textbf{input space} after the training procedure, that preserve the connections in the weights matrix.}
     \label{fig:som_weights}
\end{figure}

Self organizing maps (SOM) are a special kind of Neural Network used for unsupervised learning, to produce a \textit{low-dimensional} representation of a higher dimensional data set, preserving the topological structure of the data. \newline
This network is composed of two layers: one \textit{input} layer and one \textit{output} layer. The output layer is made of a set of weights $\mathcal{W} = \{\textbf{w}\ |\ \textbf{w} \in \mathbb{R}^m\}$, that are vectors in the same \textit{m-dimensional} space as the input vector $\textbf{p} \in \mathbb{R}^m$.
The set of weights can have different shapes, for example can be \textit{one-dimensional} and act like a line, or can be \textit{two-dimensional} with a grid or hexagonal displacement. \newline
The weights of this network lies in the same space as the input, so each weight represents a point in the space of the input vector, that is $\mathbb{R}^m$. At each iteration of the training procedure, the weights are update to become nearer to the input data: using competitive learning, the nearest weight to the input data and its neighbors are updated; after the training, the weights of the SOM will be placed near the most concentrated areas of the input data while preserving the topological structure of the weights. Indeed, weights that are neighbor in the network are also near in the \textit{m-dimensional} space.

\paragraph{}
What we need to know for our purposes is that this kind of model keeps an embedding of data in the weights, thus each weight $\textbf{w}$ is a representative of some samples in the dataset that have $\textbf{w}$ as the nearest weight. Therefore, the displacement of the weights both in the input space and in the topological space can be used to determine the patterns in the input dataset. \newline
\hspace{5pt} In figure \ref{fig:som_weights} can be seen two different SOMs trained on a $\mathcal{MSS}$ of size $\rho = 10$. In green there is the dataset, in red the weight's positions in the input space and the black lines are the connections between weights in the topology of the network. \newline
The first one is a SOM with a linear topology that represents a sort of line. After the training, the weights arranged in the input space moving towards denser regions, preserving the topology of the network thanks to the neighboring function \ref{eq:som_neighbor_func}. \newline
In the second one the SOM has a rectangular topology and the same effect can be noticed, allowing weights to fall in dense regions preserving the topology.

\section{Preference Embedding}
\paragraph{}
As said, the algorithm trains the pool of model, in which each learner is trained on a portion of the dataset and then the preference is computed. The preference in PIF paper is expressed as a function of the residual between a point and the model; in this thesis, instead, we consider the preference as a function of a quantity called \textbf{pseudo-preference} that depends on the model used, but as similarly as in the PIF's residual, therefore if a model express a low pseudo-preference for a point, the corresponding entry in the preference matrix will be higher, since is computed as a decreasing function of the pseudo-preference.

\paragraph{}
In particular, Auto-Encoders compute the preference as in PIF formulation, thus as the euclidean distance between the point and the prediction for that point. We choose this distance because is the same on which the network is trained, thus it represents the reconstruction error. Therefore, the preference of the Auto-Encoder $\mathcal{G}_j$ expressed for point $p_i$ is the following:
\begin{equation}
\label{eq:ae_preference}
    \delta_{ij} = (\textbf{p}_i - \hat{\textbf{p}}_i)^2
\end{equation}
where $\hat{\textbf{p}}_i = \mathcal{G}_j(\textbf{p}_i)$

\begin{figure}[hb]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/autoencoders/MSS_ae_after.svg}
         \caption{Set of predictions for the whole dataset, $\{ \mathcal{G}(\textbf{p}) | \textbf{p} \in \mathcal{D} \}$.}
         \label{subfig:ae_mss_preds}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/autoencoders/MSS_ae_residuals.svg}
         \caption{Set of predictions for the whole dataset, $\{ \mathcal{G}(\textbf{p}) | \textbf{p} \in \mathcal{D} \}$.}
         \label{subfig:ae_mss_res}
     \end{subfigure}
     \caption{This figure shows the pseudo-preference expressed by an auto-encoder trained on a $\mathcal{MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.}
     \label{fig:ae_residuals}
\end{figure}

\paragraph{}
Regarding Self Organizing Maps, however, express a meaningful preference from the network is not simple. We tried a lot of approaches, but we figured out that a method exploiting both the positions of the weight, i.e. position in the input space $\in \mathbb{R}^m$ and position in the topology of the weights set $\mathcal{W}$, could lead to better results. \newline
The process is composed of three steps:
\begin{enumerate}[label=(\roman*)]
    \item find, in the input space, the weight with minimum distance from $\textbf{p}$ among all the weights $\textbf{w} \in \mathcal{W}$;
    \item find the set of weights $\mathcal{Q} \subset \mathcal{W}$ that are neighbors to $\textbf{w}$ in the arrangement of $\mathcal{W}$;
    \item compute the preference for a point as the mean value of the distances between $\textbf{p}$ and $\mathcal{Q} \cup {\textbf{w}}$ in the input space.
\end{enumerate}
This computation takes advantage of the weights' natural arrangement in the network, whereby weights that are close together in the input space are likewise close together in the weights matrix's topology. Anomalies, on the other hand, are not defined by the pattern, therefore few weights will stay in its area. As a result, normal points that describe a pattern will have numerous weights in its neighborhood because during training weights were distributed there.

\begin{figure}[hb]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_line_residuals.svg}
         \caption{Linear SOM}
         \label{subfig:linear_som_res}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
        \includesvg[width=\textwidth]{Images/problem_solution/som/MSS_som_square_residuals.svg}
         \caption{Square SOM}
         \label{subfig:square_som_res}
     \end{subfigure}
     \caption{This figure shows the pseudo-preference expressed by two SOMs, one with a linear topology (fig \ref{subfig:linear_som_res}), the other with a square topology (fig \ref{subfig:square_som_res}). Both have been trained on a $\mathcal{MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.}
     \label{fig:som_residuals}
\end{figure}

\paragraph{}
As an example, in figure \ref{fig:som_residuals} are shown the \textit{pseudo-preferences} expressed by the two SOMs of figure \ref{fig:som_weights} on the same dataset. The more an instance is towards the red color, the more it has high \textit{pseudo-preference} - and so smaller preference in the matrix. Therefore, the points belonging to the patterns have a smaller pseudo-preference and are more probably normal points.
