\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\transparent@use{.4}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Proposed Method}{35}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:proposed_solution}{{4}{35}{Proposed Method}{chapter.4}{}}
\newlabel{ch:proposed_solution@cref}{{[chapter][4][]4}{[1][35][]35}}
\@writefile{toc}{\contentsline {paragraph}{}{35}{paragraph*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{35}{paragraph*.116}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{35}{paragraph*.117}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{35}{paragraph*.118}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Problem Formulation}{36}{section.4.1}\protected@file@percent }
\newlabel{sec:problem_formulation}{{4.1}{36}{Problem Formulation}{section.4.1}{}}
\newlabel{sec:problem_formulation@cref}{{[section][1][4]4.1}{[1][36][]36}}
\@writefile{toc}{\contentsline {paragraph}{}{36}{paragraph*.119}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{36}{paragraph*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{36}{paragraph*.121}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Rationale}{36}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{36}{paragraph*.122}\protected@file@percent }
\citation{ae_paper}
\citation{som_paper}
\@writefile{toc}{\contentsline {paragraph}{}{37}{paragraph*.123}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{37}{paragraph*.124}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{37}{paragraph*.125}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{38}{paragraph*.126}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Algorithm}{38}{section.4.3}\protected@file@percent }
\newlabel{sec:algorithm}{{4.3}{38}{Algorithm}{section.4.3}{}}
\newlabel{sec:algorithm@cref}{{[section][3][4]4.3}{[1][38][]38}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.1}{\ignorespaces \textit  {Neural-PIF}\relax }}{39}{algorithm.4.1}\protected@file@percent }
\newlabel{alg:npif}{{4.1}{39}{\textit {Neural-PIF}\relax }{algorithm.4.1}{}}
\newlabel{alg:npif@cref}{{[algorithm][1][4]4.1}{[1][39][]39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Pattern learners}{39}{section.4.4}\protected@file@percent }
\citation{tangent_line_approx}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Line and Plane}{40}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{40}{paragraph*.127}\protected@file@percent }
\newlabel{fig:linear_approx}{{\caption@xref {fig:linear_approx}{ on input line 125}}{40}{}{figure.caption.128}{}}
\newlabel{fig:linear_approx@cref}{{[subsection][1][4,4]4.4.1}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of a linear approximation on a generic function $y = f(x)$. In blue the true function, in green the tangent at the point $(p, f(p))$ that is marked in red.\relax }}{40}{figure.caption.128}\protected@file@percent }
\citation{plane_local_approx}
\@writefile{toc}{\contentsline {paragraph}{}{41}{paragraph*.130}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{41}{paragraph*.131}\protected@file@percent }
\citation{ae_anom_detect}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Example of a plane, $\alpha $, the is tangent to the surface $f(x, y, z)$ at the point $\textbf  {p}$.\relax }}{42}{figure.caption.135}\protected@file@percent }
\newlabel{fig:local_plane}{{4.2}{42}{Example of a plane, $\alpha $, the is tangent to the surface $f(x, y, z)$ at the point $\textbf {p}$.\relax }{figure.caption.135}{}}
\newlabel{fig:local_plane@cref}{{[figure][2][4]4.2}{[1][41][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Auto-encoders}{42}{subsection.4.4.2}\protected@file@percent }
\newlabel{subfig:ds}{{4.3a}{43}{The dataset\relax }{figure.caption.136}{}}
\newlabel{subfig:ds@cref}{{[subfigure][1][4,3]4.3a}{[1][42][]43}}
\newlabel{sub@subfig:ds}{{a}{43}{The dataset\relax }{figure.caption.136}{}}
\newlabel{sub@subfig:ds@cref}{{[subfigure][1][4,3]4.3a}{[1][42][]43}}
\newlabel{subfig:mlp}{{4.3b}{43}{MLP\relax }{figure.caption.136}{}}
\newlabel{subfig:mlp@cref}{{[subfigure][2][4,3]4.3b}{[1][42][]43}}
\newlabel{sub@subfig:mlp}{{b}{43}{MLP\relax }{figure.caption.136}{}}
\newlabel{sub@subfig:mlp@cref}{{[subfigure][2][4,3]4.3b}{[1][42][]43}}
\newlabel{subfig:ae1}{{4.3c}{43}{Auto-Encoder\relax }{figure.caption.136}{}}
\newlabel{subfig:ae1@cref}{{[subfigure][3][4,3]4.3c}{[1][42][]43}}
\newlabel{sub@subfig:ae1}{{c}{43}{Auto-Encoder\relax }{figure.caption.136}{}}
\newlabel{sub@subfig:ae1@cref}{{[subfigure][3][4,3]4.3c}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces This figure shows the utility of the constraint on the latent representation's number of dimensions. In image \ref  {subfig:mlp}, a Multi Layer Perceptron (MLP) network has learned to reconstruct the input vector generating a new vector in the same space; the problem is that this network didn't generate any latent representation with fewer dimension, thus it has learnt the identity matrix. On the contrary, in image \ref  {subfig:ae1}, an Auto-Encoder with a latent representation with less dimensions than the input space has learned a new representation of the input dataset, not the identity matrix.\relax }}{43}{figure.caption.136}\protected@file@percent }
\newlabel{fig:ae_constraint}{{4.3}{43}{This figure shows the utility of the constraint on the latent representation's number of dimensions. In image \ref {subfig:mlp}, a Multi Layer Perceptron (MLP) network has learned to reconstruct the input vector generating a new vector in the same space; the problem is that this network didn't generate any latent representation with fewer dimension, thus it has learnt the identity matrix. On the contrary, in image \ref {subfig:ae1}, an Auto-Encoder with a latent representation with less dimensions than the input space has learned a new representation of the input dataset, not the identity matrix.\relax }{figure.caption.136}{}}
\newlabel{fig:ae_constraint@cref}{{[figure][3][4]4.3}{[1][42][]43}}
\@writefile{toc}{\contentsline {paragraph}{}{43}{paragraph*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{43}{paragraph*.138}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{44}{paragraph*.139}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Relationship between auto-encoders and lines}{44}{subsubsection*.140}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{44}{paragraph*.141}\protected@file@percent }
\newlabel{eq:x_hat}{{4.6}{44}{}{equation.4.4.6}{}}
\newlabel{eq:x_hat@cref}{{[equation][6][4]4.6}{[1][44][]44}}
\newlabel{eq:y_hat}{{4.7}{44}{}{equation.4.4.7}{}}
\newlabel{eq:y_hat@cref}{{[equation][7][4]4.7}{[1][44][]44}}
\citation{line_between_two_points}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Structure of the network used in this section. The pedices of the weights are the starting neuron's index and the connected neuron's index. So, for example, weight $w_{02}$ goes from neuron $n_0$ to neuron $n_2$, or weight $w_{23}$ goes from neuron $n_2$ to neuron $n_3$. The biases have the same index to which they belong; for example, bias $b_4$ is the bias of neuron $n_4$.\relax }}{45}{figure.caption.142}\protected@file@percent }
\newlabel{fig:ae_212}{{4.4}{45}{Structure of the network used in this section. The pedices of the weights are the starting neuron's index and the connected neuron's index. So, for example, weight $w_{02}$ goes from neuron $n_0$ to neuron $n_2$, or weight $w_{23}$ goes from neuron $n_2$ to neuron $n_3$. The biases have the same index to which they belong; for example, bias $b_4$ is the bias of neuron $n_4$.\relax }{figure.caption.142}{}}
\newlabel{fig:ae_212@cref}{{[figure][4][4]4.4}{[1][44][]45}}
\@writefile{toc}{\contentsline {paragraph}{}{45}{paragraph*.146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{46}{paragraph*.151}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{47}{paragraph*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Relationship between auto-encoders and planes}{47}{subsubsection*.157}\protected@file@percent }
\newlabel{subsec:ae-planes}{{4.4.2}{47}{Relationship between auto-encoders and planes}{subsubsection*.157}{}}
\newlabel{subsec:ae-planes@cref}{{[subsection][2][4,4]4.4.2}{[1][47][]47}}
\@writefile{toc}{\contentsline {paragraph}{}{47}{paragraph*.158}\protected@file@percent }
\citation{plane_between_three_points}
\@writefile{toc}{\contentsline {paragraph}{}{48}{paragraph*.159}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The structure of the network used in this section. The convention for naming the weights and biases is the same used in figure \ref  {fig:ae_212}.\relax }}{48}{figure.caption.160}\protected@file@percent }
\newlabel{fig:ae_323}{{4.5}{48}{The structure of the network used in this section. The convention for naming the weights and biases is the same used in figure \ref {fig:ae_212}.\relax }{figure.caption.160}{}}
\newlabel{fig:ae_323@cref}{{[figure][5][4]4.5}{[1][47][]48}}
\newlabel{eq:plane_estim}{{4.21}{49}{}{equation.4.4.21}{}}
\newlabel{eq:plane_estim@cref}{{[equation][21][4]4.21}{[1][49][]49}}
\newlabel{eq:plane_coefficients}{{4.22}{49}{}{equation.4.4.22}{}}
\newlabel{eq:plane_coefficients@cref}{{[equation][22][4]4.22}{[1][49][]49}}
\newlabel{eq:plane_z_explicit}{{4.24}{50}{}{equation.4.4.24}{}}
\newlabel{eq:plane_z_explicit@cref}{{[equation][24][4]4.24}{[1][50][]50}}
\@writefile{toc}{\contentsline {paragraph}{}{50}{paragraph*.170}\protected@file@percent }
\newlabel{eq:ae_323_points}{{4.25}{50}{}{equation.4.4.25}{}}
\newlabel{eq:ae_323_points@cref}{{[equation][25][4]4.25}{[1][50][]50}}
\@writefile{toc}{\contentsline {paragraph}{}{51}{paragraph*.173}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{51}{paragraph*.175}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Self Organizing Maps}{51}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{52}{paragraph*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Preference Embedding}{52}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{52}{paragraph*.178}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{52}{paragraph*.179}\protected@file@percent }
\newlabel{eq:ae_preference}{{4.28}{52}{}{equation.4.5.28}{}}
\newlabel{eq:ae_preference@cref}{{[equation][28][4]4.28}{[1][52][]52}}
\@writefile{toc}{\contentsline {paragraph}{}{52}{paragraph*.182}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{53}{paragraph*.184}\protected@file@percent }
\newlabel{subfig:som-one-before}{{4.6a}{54}{\textit {1D} before training\relax }{figure.caption.176}{}}
\newlabel{subfig:som-one-before@cref}{{[subfigure][1][4,6]4.6a}{[1][51][]54}}
\newlabel{sub@subfig:som-one-before}{{a}{54}{\textit {1D} before training\relax }{figure.caption.176}{}}
\newlabel{sub@subfig:som-one-before@cref}{{[subfigure][1][4,6]4.6a}{[1][51][]54}}
\newlabel{subfig:som-one-after}{{4.6b}{54}{\textit {1D} after training\relax }{figure.caption.176}{}}
\newlabel{subfig:som-one-after@cref}{{[subfigure][2][4,6]4.6b}{[1][51][]54}}
\newlabel{sub@subfig:som-one-after}{{b}{54}{\textit {1D} after training\relax }{figure.caption.176}{}}
\newlabel{sub@subfig:som-one-after@cref}{{[subfigure][2][4,6]4.6b}{[1][51][]54}}
\newlabel{subfig:som-two-before}{{4.6c}{54}{\textit {2D} before training\relax }{figure.caption.176}{}}
\newlabel{subfig:som-two-before@cref}{{[subfigure][3][4,6]4.6c}{[1][51][]54}}
\newlabel{sub@subfig:som-two-before}{{c}{54}{\textit {2D} before training\relax }{figure.caption.176}{}}
\newlabel{sub@subfig:som-two-before@cref}{{[subfigure][3][4,6]4.6c}{[1][51][]54}}
\newlabel{subfig:som-two-after}{{4.6d}{54}{\textit {2D} after training\relax }{figure.caption.176}{}}
\newlabel{subfig:som-two-after@cref}{{[subfigure][4][4,6]4.6d}{[1][51][]54}}
\newlabel{sub@subfig:som-two-after}{{d}{54}{\textit {2D} after training\relax }{figure.caption.176}{}}
\newlabel{sub@subfig:som-two-after@cref}{{[subfigure][4][4,6]4.6d}{[1][51][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Two examples of how SOM weights can be arranged. Red dots are weights in the input space, while the black lines are the connections between weights in the topology of the network. In plots \ref  {subfig:som-one-before} and \ref  {subfig:som-two-before} are shown the initial weights, representing also the arrangement of the weights matrix. Remember that in general, the displacement in the input space is different from the displacement in weight's matrix. In figures \ref  {subfig:som-one-after} and \ref  {subfig:som-two-after} show the displacement of the weights in the \textbf  {input space} after the training procedure, that preserve the connections in the weights matrix.\relax }}{54}{figure.caption.176}\protected@file@percent }
\newlabel{fig:som_weights}{{4.6}{54}{Two examples of how SOM weights can be arranged. Red dots are weights in the input space, while the black lines are the connections between weights in the topology of the network. In plots \ref {subfig:som-one-before} and \ref {subfig:som-two-before} are shown the initial weights, representing also the arrangement of the weights matrix. Remember that in general, the displacement in the input space is different from the displacement in weight's matrix. In figures \ref {subfig:som-one-after} and \ref {subfig:som-two-after} show the displacement of the weights in the \textbf {input space} after the training procedure, that preserve the connections in the weights matrix.\relax }{figure.caption.176}{}}
\newlabel{fig:som_weights@cref}{{[figure][6][4]4.6}{[1][51][]54}}
\newlabel{subfig:ae_mss_preds}{{4.7a}{54}{Set of predictions for the whole dataset, $\{ \mathcal {G}(\textbf {p}) | \textbf {p} \in \mathcal {D} \}$.\relax }{figure.caption.181}{}}
\newlabel{subfig:ae_mss_preds@cref}{{[subfigure][1][4,7]4.7a}{[1][52][]54}}
\newlabel{sub@subfig:ae_mss_preds}{{a}{54}{Set of predictions for the whole dataset, $\{ \mathcal {G}(\textbf {p}) | \textbf {p} \in \mathcal {D} \}$.\relax }{figure.caption.181}{}}
\newlabel{sub@subfig:ae_mss_preds@cref}{{[subfigure][1][4,7]4.7a}{[1][52][]54}}
\newlabel{subfig:ae_mss_res}{{4.7b}{54}{Set of predictions for the whole dataset, $\{ \mathcal {G}(\textbf {p}) | \textbf {p} \in \mathcal {D} \}$.\relax }{figure.caption.181}{}}
\newlabel{subfig:ae_mss_res@cref}{{[subfigure][2][4,7]4.7b}{[1][52][]54}}
\newlabel{sub@subfig:ae_mss_res}{{b}{54}{Set of predictions for the whole dataset, $\{ \mathcal {G}(\textbf {p}) | \textbf {p} \in \mathcal {D} \}$.\relax }{figure.caption.181}{}}
\newlabel{sub@subfig:ae_mss_res@cref}{{[subfigure][2][4,7]4.7b}{[1][52][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces This figure shows the pseudo-preference expressed by an auto-encoder trained on a $\mathcal  {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }}{54}{figure.caption.181}\protected@file@percent }
\newlabel{fig:ae_residuals}{{4.7}{54}{This figure shows the pseudo-preference expressed by an auto-encoder trained on a $\mathcal {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }{figure.caption.181}{}}
\newlabel{fig:ae_residuals@cref}{{[figure][7][4]4.7}{[1][52][]54}}
\newlabel{subfig:linear_som_res}{{4.8a}{55}{Linear SOM\relax }{figure.caption.183}{}}
\newlabel{subfig:linear_som_res@cref}{{[subfigure][1][4,8]4.8a}{[1][53][]55}}
\newlabel{sub@subfig:linear_som_res}{{a}{55}{Linear SOM\relax }{figure.caption.183}{}}
\newlabel{sub@subfig:linear_som_res@cref}{{[subfigure][1][4,8]4.8a}{[1][53][]55}}
\newlabel{subfig:square_som_res}{{4.8b}{55}{Square SOM\relax }{figure.caption.183}{}}
\newlabel{subfig:square_som_res@cref}{{[subfigure][2][4,8]4.8b}{[1][53][]55}}
\newlabel{sub@subfig:square_som_res}{{b}{55}{Square SOM\relax }{figure.caption.183}{}}
\newlabel{sub@subfig:square_som_res@cref}{{[subfigure][2][4,8]4.8b}{[1][53][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces This figure shows the pseudo-preference expressed by two SOMs, one with a linear topology (fig \ref  {subfig:linear_som_res}), the other with a square topology (fig \ref  {subfig:square_som_res}). Both have been trained on a $\mathcal  {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }}{55}{figure.caption.183}\protected@file@percent }
\newlabel{fig:som_residuals}{{4.8}{55}{This figure shows the pseudo-preference expressed by two SOMs, one with a linear topology (fig \ref {subfig:linear_som_res}), the other with a square topology (fig \ref {subfig:square_som_res}). Both have been trained on a $\mathcal {MSS}$ of size $\rho = 10$. The higher the value (red), the lower the preference.\relax }{figure.caption.183}{}}
\newlabel{fig:som_residuals@cref}{{[figure][8][4]4.8}{[1][53][]55}}
\@setckpt{chapters/proposed_solution}{
\setcounter{page}{56}
\setcounter{equation}{28}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{thmt@dummyctr}{0}
\setcounter{float@type}{32}
\setcounter{parentequation}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{32}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{1}
\setcounter{FancyVerbLine}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{69}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{minted@FancyVerbLineTemp}{0}
\setcounter{minted@pygmentizecounter}{0}
\setcounter{listing}{0}
\setcounter{Item}{19}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{36}
\setcounter{NAT@ctr}{0}
\setcounter{AM@survey}{0}
\setcounter{svg@param@lastpage}{0}
\setcounter{svg@param@currpage}{-1}
\setcounter{theorem}{0}
\setcounter{proposition}{0}
\setcounter{algsubstate}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
